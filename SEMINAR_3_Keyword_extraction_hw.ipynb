{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\Program Files\\Anaconda3\\Lib\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "morph = MorphAnalyzer()\n",
    "stops = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем данные вот отсюда - https://github.com/mannefedov/ru_kw_eval_datasets Там лежат 4 датасета (статьи с хабра, с Russia Today, Независимой газеты и научные статьи с Киберленинки). Датасет НГ самый маленький, поэтому возьмем его в качестве примера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# скачаем данные в папке data и распакуем их\n",
    "PATH_TO_DATA = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [file.replace('\\\\','/') for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/ng_0.jsonlines', './data/ng_1.jsonlines']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим файлы в один датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([pd.read_json(file, lines=True, encoding = 'utf-8') for file in files], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1987, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Многие интересуются, зачем нужна «Яблоку» молодежная фракция? Основной задачей «Молодежного «Яблока» является привлечение молодых людей к участию в выборах и деятельности партии. «Молодежное «Яблоко» работает более чем в 10 регионах. Единого руководства у нас нет, но мы стараемся координировать свою деятельность и периодически проводим акции на федеральном уровне.\\nМы ведем борьбу с обязательным воинским призывом. Военный – это профессия, а не обязанность. Молодые люди вправе сами распоряжаться своей жизнью и не терять целый год, отдавая государству «долг», который они у него не занимали. По мнению одного из ведущих специалистов в области оборонной политики Алексея Арбатова, переход на контрактную армию будет стоить лишь 2% военного бюджета.\\nТакже на федеральном уровне «Молодежное «Яблоко» проводило акции за освобождение политзаключенных и против вмешательства России во внутреннюю политику Украины.\\nРасскажу о московских активистах. Виктору Петрунину – 19 лет, он пришел к нам боль...</td>\n",
       "      <td>[яблоко, молодежь, молодежное яблоко]</td>\n",
       "      <td></td>\n",
       "      <td>\"Молодежное \"Яблоко\": оппозиционная деятельность становится опасной</td>\n",
       "      <td>http://www.ng.ru/ng_politics/2017-04-18/11_6976_apple.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вчера «Газпром» снизил верхнюю планку прогноза собственной добычи газа в 2020 году. Через 12 лет концерн собирается добывать около 620–640 млрд. куб. м в год. При этом общее производство газа в стране, по расчетам холдинга, должно достичь 940 млрд. куб. м. Иными словами, треть добываемого объема, по мнению холдинга, должны будут обеспечить независимые производители. Эксперты не верят, что независимые компании смогут выйти на такие объемы добычи. Если расчеты «Газпрома» не оправдаются, то под ударом окажутся отечественные предприятия и население, которым придется сокращать потребление и смириться с новым витком цен. Иных путей покрытия возможного дефицита газа нет, так как вряд ли холдинг разорвет уже заключенные контракты на экспорт газа в другие страны. \\n«Газпром» к 2020 году планирует добывать 620–640 млрд. куб. м газа, сообщил вчера на форуме «ТЭК России в ХХI веке» глава управления по добыче газа, газового конденсата и нефти холдинга Валерий Минликаев. Тем самым он уточнил пре...</td>\n",
       "      <td>[газпром, газ]</td>\n",
       "      <td></td>\n",
       "      <td>\"Газпрома\" на всех не хватит</td>\n",
       "      <td>http://www.ng.ru/economics/2008-04-03/1_gazprom.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Долголетний труд Евгения Витковского на ниве перевода, а также в качестве редактора и антологиста известен многим. Но не все знают его как поэта и прозаика. В этом году уже вышла составленная им и Еленой Кистеровой антология  «Раздол туманов: Страницы шотландской гэльской поэзии XVII–XX вв.», а в апреле запланирован выход его романа «Протей, или Византийский кризис» (отрывок из романа читайте на с. 12). С \\n побеседовал \\n– Одна из таких книг только что вышла – «Раздол туманов. Страницы шотландской гэльской поэзии XVII–XX веков». Это стихи 29 поэтов, все в переводе с оригинала – моем и Елены Кистеровой. Работа заняла 10 лет, включая изучение языка. Она была упоительно интересной: до нас переводов из этой поэзии на русский не было вовсе. Сейчас должен выйти том стихотворений канадского классика Роберта Уильяма Сервиса, «канадского Киплинга», около 300 стихотворений. Кроме того, в Петербурге в производстве наш огромный трехтомный плод совместной работы – антология «Франция в сердце»....</td>\n",
       "      <td>[франсуа рабле, сервантес, шекспир, конан дойл, михаил булгаков, александр грин, борхес, босх, маркес, герман гессе, голландская живопись, гаргантюа и пантагрюэль, дон кихот, мастер и маргарита, москва, россия, история, поэзия, шотландия, баллада, пере]</td>\n",
       "      <td>Евгений Витковский о том, как Босх протягивает руку Шекспиру, \\r\\nи оба танцуют в пламени пожара в охваченном чумой средневековом городе</td>\n",
       "      <td>Бесконечная партия в четырехмерные  шахматы</td>\n",
       "      <td>http://www.ng.ru/person/2018-03-22/10_927_vitkovsky.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   content  \\\n",
       "0  Многие интересуются, зачем нужна «Яблоку» молодежная фракция? Основной задачей «Молодежного «Яблока» является привлечение молодых людей к участию в выборах и деятельности партии. «Молодежное «Яблоко» работает более чем в 10 регионах. Единого руководства у нас нет, но мы стараемся координировать свою деятельность и периодически проводим акции на федеральном уровне.\\nМы ведем борьбу с обязательным воинским призывом. Военный – это профессия, а не обязанность. Молодые люди вправе сами распоряжаться своей жизнью и не терять целый год, отдавая государству «долг», который они у него не занимали. По мнению одного из ведущих специалистов в области оборонной политики Алексея Арбатова, переход на контрактную армию будет стоить лишь 2% военного бюджета.\\nТакже на федеральном уровне «Молодежное «Яблоко» проводило акции за освобождение политзаключенных и против вмешательства России во внутреннюю политику Украины.\\nРасскажу о московских активистах. Виктору Петрунину – 19 лет, он пришел к нам боль...   \n",
       "1  Вчера «Газпром» снизил верхнюю планку прогноза собственной добычи газа в 2020 году. Через 12 лет концерн собирается добывать около 620–640 млрд. куб. м в год. При этом общее производство газа в стране, по расчетам холдинга, должно достичь 940 млрд. куб. м. Иными словами, треть добываемого объема, по мнению холдинга, должны будут обеспечить независимые производители. Эксперты не верят, что независимые компании смогут выйти на такие объемы добычи. Если расчеты «Газпрома» не оправдаются, то под ударом окажутся отечественные предприятия и население, которым придется сокращать потребление и смириться с новым витком цен. Иных путей покрытия возможного дефицита газа нет, так как вряд ли холдинг разорвет уже заключенные контракты на экспорт газа в другие страны. \\n«Газпром» к 2020 году планирует добывать 620–640 млрд. куб. м газа, сообщил вчера на форуме «ТЭК России в ХХI веке» глава управления по добыче газа, газового конденсата и нефти холдинга Валерий Минликаев. Тем самым он уточнил пре...   \n",
       "2  Долголетний труд Евгения Витковского на ниве перевода, а также в качестве редактора и антологиста известен многим. Но не все знают его как поэта и прозаика. В этом году уже вышла составленная им и Еленой Кистеровой антология  «Раздол туманов: Страницы шотландской гэльской поэзии XVII–XX вв.», а в апреле запланирован выход его романа «Протей, или Византийский кризис» (отрывок из романа читайте на с. 12). С \\n побеседовал \\n– Одна из таких книг только что вышла – «Раздол туманов. Страницы шотландской гэльской поэзии XVII–XX веков». Это стихи 29 поэтов, все в переводе с оригинала – моем и Елены Кистеровой. Работа заняла 10 лет, включая изучение языка. Она была упоительно интересной: до нас переводов из этой поэзии на русский не было вовсе. Сейчас должен выйти том стихотворений канадского классика Роберта Уильяма Сервиса, «канадского Киплинга», около 300 стихотворений. Кроме того, в Петербурге в производстве наш огромный трехтомный плод совместной работы – антология «Франция в сердце»....   \n",
       "\n",
       "                                                                                                                                                                                                                                                        keywords  \\\n",
       "0                                                                                                                                                                                                                          [яблоко, молодежь, молодежное яблоко]   \n",
       "1                                                                                                                                                                                                                                                 [газпром, газ]   \n",
       "2  [франсуа рабле, сервантес, шекспир, конан дойл, михаил булгаков, александр грин, борхес, босх, маркес, герман гессе, голландская живопись, гаргантюа и пантагрюэль, дон кихот, мастер и маргарита, москва, россия, история, поэзия, шотландия, баллада, пере]   \n",
       "\n",
       "                                                                                                                                    summary  \\\n",
       "0                                                                                                                                             \n",
       "1                                                                                                                                             \n",
       "2  Евгений Витковский о том, как Босх протягивает руку Шекспиру, \\r\\nи оба танцуют в пламени пожара в охваченном чумой средневековом городе   \n",
       "\n",
       "                                                                 title  \\\n",
       "0  \"Молодежное \"Яблоко\": оппозиционная деятельность становится опасной   \n",
       "1                                         \"Газпрома\" на всех не хватит   \n",
       "2                          Бесконечная партия в четырехмерные  шахматы   \n",
       "\n",
       "                                                          url  \n",
       "0  http://www.ng.ru/ng_politics/2017-04-18/11_6976_apple.html  \n",
       "1        http://www.ng.ru/economics/2008-04-03/1_gazprom.html  \n",
       "2    http://www.ng.ru/person/2018-03-22/10_927_vitkovsky.html  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждой статье приписано какое-то количество ключевых слов. Допустим, что это единственно правильный набор ключевых слов (что конечно не так, но других данных у нас нет). Наша задача - придумать как извлекать точно такой же список автоматически.  \n",
    "Зададим несколько метрик, по которым будем определять качество извлекаемых ключевых слов - точность, полноту, ф1-меру и меру жаккарда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp+fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp+fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что всё работает как надо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('data.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  1.0\n",
      "Recall -  1.0\n",
      "F1 -  1.0\n",
      "Jaccard -  1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тупое решение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте не будем думать, а попробуем сразу придумать какое-то решение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем первые 5 слов из заголовка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.06\n",
      "Recall -  0.05\n",
      "F1 -  0.05\n",
      "Jaccard -  0.03\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['title'].apply(lambda x: x.lower().split()[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.06\n",
      "Recall -  0.06\n",
      "F1 -  0.05\n",
      "Jaccard -  0.03\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['title'].apply(lambda x: x.lower().split()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем взять самые частотные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.02\n",
      "Recall -  0.04\n",
      "F1 -  0.02\n",
      "Jaccard -  0.01\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content'].apply(lambda x: \n",
    "                                                 [x[0] for x in Counter(x.lower().split()).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или вообще рандомные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.01\n",
      "Recall -  0.01\n",
      "F1 -  0.01\n",
      "Jaccard -  0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content'].apply(lambda x: \n",
    "                                                 np.random.choice(list(set(x.lower().split())), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим, что вообще извлекается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         [\"молодежное, \"яблоко\":, оппозиционная, деятельность, становится, опасной]\n",
       "1                                                                 [\"газпрома\", на, всех, не, хватит]\n",
       "2                                                   [бесконечная, партия, в, четырехмерные, шахматы]\n",
       "3    [экс-депутат,, осужденная, за, фальсификацию, выборов,, оказалась, членом, \"боевого, братства\"]\n",
       "4                               [новая, москва, останется, территорией, экологической, безопасности]\n",
       "5                                [f1., гран-при, сша, прошел, без, четырех, машин, и, со, «стопкой»]\n",
       "6                                          [100, ведущих, политиков, россии, в, феврале, 2018, года]\n",
       "7                                               [закон, \"о, культуре\", принимают, на, фоне, арестов]\n",
       "8                                    [насколько, реальна, газовая, подоплека, сирийского, конфликта]\n",
       "9                                  [фсб:, в, калужской, области, задержаны, четверо, участников, иг]\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'].apply(lambda x: x.lower().split()[:10]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    [в, и, на, что, не, –, его, а, это, из]\n",
       "1                                                 [в, и, на, –, куб., млрд., по, к, газа, м]\n",
       "2                                                      [в, –, и, не, я, но, это, что, на, с]\n",
       "3                                                 [в, на, и, ким, –, по, что, он, видео, до]\n",
       "4                                   [в, и, на, площадью, новой, москвы, –, развития, с, га.]\n",
       "5                                                  [в, на, и, не, с, но, уже, что, гонки, у]\n",
       "6                                       [на, в, (с, место)., и, рф, влияние, позиции, по, с]\n",
       "7                                             [в, и, –, по, с, как, будет, культуре, из, не]\n",
       "8                                                   [в, и, на, с, что, для, по, –, не, газа]\n",
       "9    [в, террористической, организации, задержаны, рф, –, боевую, цос,, исламистов, жителей]\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content'].apply(lambda x: [x[0] for x in Counter(x.lower().split()).most_common(10)]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда извлекаются частотные слова, то список почти полностью состоит из всяких стоп-слов. Также из-за плохой токенизации некоторые слова в обоих списках - пунктуация или слова с пунктуацией на концах. К тому же извлекаемые слова ненормализованы, а правильные ключевые слова - наоборот."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация, удаление стоп-слов и нормализация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "stopwords = get_stop_words('russian')\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    #words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = re.findall(r'[a-zа-яё]+', text.lower())\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stopwords]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Изменим немного функцию предобработки, чтобы оставить только буквенные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['title_norm'] = data['title'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [молодёжный, яблоко, оппозиционный, деятельность, становиться, опасный]\n",
       "1                                                                  [газпром, хватить]\n",
       "2                                       [бесконечный, партия, четырехмерный, шахматы]\n",
       "3    [экс, депутат, осудить, фальсификация, выбор, оказаться, член, боевой, братство]\n",
       "4                  [новый, москва, остаться, территория, экологический, безопасность]\n",
       "5                               [f, гран, сша, пройти, четыре, машина, стопка, штраф]\n",
       "6                                                 [ведущий, политик, россия, февраль]\n",
       "7                                            [закон, культура, принимать, фон, арест]\n",
       "8                      [насколько, реальный, газовый, подоплёка, сирийский, конфликт]\n",
       "9                        [фсб, калужский, область, задержать, четверо, участник, иго]\n",
       "Name: title_norm, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title_norm'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем те же самые методы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.23\n",
      "F1 -  0.15\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "# топ 10 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.14\n",
      "Recall -  0.13\n",
      "F1 -  0.12\n",
      "Jaccard -  0.07\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'],data['title_norm'].apply(lambda x: x[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.92\n",
      "Recall -  0.39\n",
      "F1 -  0.55\n",
      "Jaccard -  0.38\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сильно улучшилось! Можно теперь ещё раз посмотреть, что плохого извлекается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [яблоко, молодёжный, акция, активист, деятельность, политика, московский, власть, уровень, проводить]\n",
       "1              [миллиард, газа, куб, газпром, добыча, производитель, страна, независимый, внутренний, должный]\n",
       "2                                [книга, роман, писать, выйти, век, стихотворение, перевод, жанр, мир, издать]\n",
       "3                          [ким, зинаида, видео, журналист, бывший, рубль, судебный, заседание, дело, футиный]\n",
       "4                              [площадь, москва, га, новый, территория, тинао, столица, развитие, парка, парк]\n",
       "5                               [гонка, позиция, команда, место, круг, чемпионат, два, бокс, пилот, следующий]\n",
       "6                            [место, го, влияние, рф, позиция, глава, политический, россия, президент, сергей]\n",
       "7              [культура, закон, стд, сфера, общественный, услуга, проект, учреждение, сообщество, предложить]\n",
       "8                        [газопровод, сирия, турция, турецкий, газа, европа, катар, россия, российский, поток]\n",
       "9           [участник, террористический, задержать, рф, цос, организация, эмиссар, ямало, данные, государство]\n",
       "10                              [тихон, стихомант, журибеда, старик, казак, сэмена, голос, отец, дежнев, один]\n",
       "11                       [жильё, кв, миллион, жилищный, население, строительство, спрос, прогноз, доход, рост]\n",
       "12            [российский, кино, зритель, театр, хороший, получить, исследование, фильм, результат, аудитория]\n",
       "13                   [фестиваль, революция, ленин, фильм, награда, хороший, роль, посмотреть, делать, сахалин]\n",
       "14                 [фёдоров, вечер, книга, балашов, женщина, произведение, виктория, писатель, герой, женский]\n",
       "15     [сирийский, переговоры, резолюция, оон, франция, макрон, сторона, гуманитарный, президент, французский]\n",
       "16                                     [суд, кс, рф, запрос, юкос, решение, акционер, минюст, бывший, человек]\n",
       "17    [электромобиль, тысяча, кобальт, компания, автомобиль, аккумулятор, цена, машина, агентство, электрокар]\n",
       "18                          [фильм, хороший, оскар, женщина, главный, церемония, один, статуэтка, приз, актёр]\n",
       "19                              [нюрбургринг, сабина, трасса, участие, fia, тс, гонка, пилотесса, шмитц, wtcc]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё остались некоторые стоп-слова. Вместо того, чтобы расширять список, давайте попробуем выкинуть несуществительные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "#     words = [word.strip(punct) for word in text.lower().split()]\n",
    "#     words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    \n",
    "    words = re.findall(r'[a-zа-яё]+', text.lower())\n",
    "   \n",
    "    parsed = [morph.parse(word)[0] for word in words if word and word not in stopwords and len(word)>2]\n",
    "    words = [parsed_word.normal_form for parsed_word in parsed if parsed_word.tag.POS == 'NOUN']\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('data_norm.csv', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.25\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещу улучшения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [яблоко, акция, активист, деятельность, политика, власть, россия, задача, дарья, жизнь]\n",
       "1                     [миллиард, газа, куб, газпром, добыча, производитель, страна, прогноз, холдинг, расчёт]\n",
       "2                             [книга, роман, жанр, стихотворение, мир, читатель, перевод, россия, герой, том]\n",
       "3                          [ким, зинаида, видео, журналист, процесс, дело, рубль, экспертиза, суд, свидетель]\n",
       "4                        [площадь, москва, территория, парк, развитие, парка, тинао, столица, регион, власть]\n",
       "5                                [гонка, команда, позиция, место, круг, пилот, бокс, чемпионат, баттон, балл]\n",
       "6                     [место, влияние, позиция, глава, президент, сергей, россия, участие, рейтинг, институт]\n",
       "7             [культура, закон, сфера, учреждение, сообщество, услуга, проект, концепция, изменение, человек]\n",
       "8                               [газопровод, сирия, турция, газа, катар, европа, россия, поток, иран, проект]\n",
       "9    [участник, организация, центр, область, подготовка, прошедшее, ячейка, деятельность, территория, житель]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень значимые слова все ещё остались. Давайте попробуем отсеять стоп-слова с помощью tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# можно заодно сделать нграммы\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем наши тексты в векторы, где на позиции i стоит tfidf коэффициент слова i из словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_vectors = tfidf.transform(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем векторы текстов по этим коэффициентам и возьмем топ-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сортировка по убыванию, поэтому нужно развернуть список\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['яблоко',\n",
       "  'активист',\n",
       "  'акция',\n",
       "  'дарья',\n",
       "  'деятельность',\n",
       "  'политика',\n",
       "  'виктор',\n",
       "  'тимур',\n",
       "  'репрессия',\n",
       "  'выборы'],\n",
       " ['миллиард куб',\n",
       "  'куб',\n",
       "  'газпром',\n",
       "  'газа',\n",
       "  'миллиард',\n",
       "  'добыча',\n",
       "  'добыча газа',\n",
       "  'холдинг',\n",
       "  'производитель',\n",
       "  'прогноз'],\n",
       " ['роман',\n",
       "  'книга',\n",
       "  'жанр',\n",
       "  'стихотворение',\n",
       "  'читатель',\n",
       "  'том',\n",
       "  'перевод',\n",
       "  'поэзия',\n",
       "  'произведение',\n",
       "  'туманов']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.25\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат ещё немного улучшился. Немного подросла точность. Теперь вместо стоп-слов в ключевые попадают имена и все такое. Иногда это хорощо, а иногда нет (собянин - может быть ключевым словом, а дарья - вряд ли)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем этот результат за baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision -  0.13\n",
    "Recall -  0.24\n",
    "F1 -  0.16\n",
    "Jaccard -  0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем графы!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая часть методов для извлечения ключевых слов основана на применении графов. Основная идея - каким-то образом перевести текст в граф, а затем каким-то образом расчитать важность каждого узла и вывести топ-N самых важных узлов.  \n",
    "\n",
    "Перевод текста в граф -  не тривиальная задача. Часто применяют такой подход - построим матрицу совстречаемости слов (в каком-то окне), эта матрица будет нашей матрицей смежности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выбора важных узлов часто используют простой randow walk. Алгоритм примерно такой:  \n",
    "1) Каким-то образом выбирается первый узел графа (например, случайно из равномерного распределения)  \n",
    "2) на основе связей этого узла с другими, выбирается следующий узел  \n",
    "3) шаг два повторяется некоторое количество раз (например, тысячу) __*чтобы не зацикливаться, с какой-то вероятностью мы случайно перескакиваем на другой узел (даже если он никак не связан с текущим, как в шаге 1)__  \n",
    "5) на каждом шаге мы сохраняем узел в котором находимся  \n",
    "6) в конце мы считаем в каких узлах мы были чаще всего и выводим top-N  \n",
    "\n",
    "\n",
    "Предполагается, что мы часто будем приходить в важные узлы графа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наглядности реализуем этот подход без networkx. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kws(text, top=5, window_size=5, random_p=0.1):\n",
    "\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    # нормализуем строки, чтобы получилась вероятность перехода\n",
    "    for i in range(m.shape[0]):\n",
    "        m[i] /= np.sum(m[i])\n",
    "    \n",
    "    # случайно выберем первое слова, а затем будет выбирать на основе полученых распределений\n",
    "    # сделаем так 5 раз и добавим каждое слово в счетчик\n",
    "    # чтобы не забиться в одном круге, иногда будет перескакивать на случайное слово\n",
    "    \n",
    "    c = Counter()\n",
    "    # начнем с абсолютного случайно выбранного элемента\n",
    "    n = np.random.choice(len(vocab))\n",
    "    for i in range(500): # если долго считается, можно уменьшить число проходов\n",
    "        \n",
    "        # c вероятностью random_p \n",
    "        # перескакиваем на другой узел\n",
    "        go_random = np.random.choice([0, 1], p=[1-random_p, random_p])\n",
    "        if go_random:\n",
    "            n = np.random.choice(len(vocab))\n",
    "        \n",
    "        n = take_step(n, m)\n",
    "        # записываем узлы, в которых были\n",
    "        c.update([n])\n",
    "    \n",
    "    # вернем топ-N наиболее часто встретившихся сл\n",
    "    return [id2word[i] for i, count in c.most_common(top)]\n",
    "\n",
    "def take_step(n, matrix):\n",
    "    rang = len(matrix[n])\n",
    "    # выбираем узел из заданного интервала, на основе распределения из матрицы совстречаемости\n",
    "    next_n = np.random.choice(range(rang), p=matrix[n])\n",
    "    return next_n\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keywords_rw = data['content_norm'].apply(lambda x: get_kws(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.11\n",
      "Recall -  0.22\n",
      "F1 -  0.14\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [яблоко, активист, россия, женщина, власть, виктор, сторона, акция, выборы, искренность]\n",
       "1       [газпром, миллиард, куб, газа, добыча, страна, производитель, прогноз, поставка, потребление]\n",
       "2                         [роман, книга, жанр, мир, мениппея, герой, павел, поэзия, история, прозаик]\n",
       "3             [ким, зинаида, свидетель, рубль, видео, тысяча, экспертиза, прошение, работа, защитник]\n",
       "4                       [площадь, парка, москва, территория, столица, парк, регион, вид, тинао, зона]\n",
       "5                         [гонка, команда, место, баттон, круг, позиция, больница, сша, заезд, перес]\n",
       "6             [место, позиция, влияние, сообщение, глава, сергей, участие, президент, политолог, фон]\n",
       "7          [культура, закон, сфера, учёт, сообщество, учреждение, человек, вопрос, взгляд, изменение]\n",
       "8                     [газопровод, катар, газа, поток, турция, сирия, европа, поставка, россия, иран]\n",
       "9    [участник, область, организация, ячейка, центр, государство, подготовка, март, сообщение, связь]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_rw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попбруем теперь важность считать с помощью какой-нибудь метрики из networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_matrix(text, window_size=5):\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    return m, id2word\n",
    "\n",
    "def some_centrality_measure(text, window_size=5, topn=5):\n",
    "    \n",
    "    matrix, id2word = build_matrix(text, window_size)\n",
    "    G = nx.from_numpy_matrix(matrix)\n",
    "    # тут можно поставить любую метрику\n",
    "    node2measure = dict(nx.degree(G))\n",
    "    \n",
    "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keyword_nx = data['content_norm'].apply(lambda x: some_centrality_measure(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.25\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keyword_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [яблоко, акция, активист, деятельность, политика, власть, дарья, жизнь, россия, задача]\n",
       "1                [газа, газпром, миллиард, куб, добыча, производитель, холдинг, прогноз, расчёт, страна]\n",
       "2                     [книга, роман, читатель, жанр, мир, стихотворение, перевод, мениппея, работа, том]\n",
       "3                [ким, зинаида, видео, процесс, свидетель, рубль, экспертиза, заседание, журналист, суд]\n",
       "4              [москва, площадь, территория, столица, тинао, регион, развитие, парка, парк, департамент]\n",
       "5                           [гонка, позиция, команда, место, круг, чемпионат, пилот, заезд, льюис, бокс]\n",
       "6                     [место, влияние, позиция, глава, россия, президент, сергей, участие, рейтинг, фон]\n",
       "7         [культура, закон, сфера, учреждение, сообщество, концепция, изменение, дело, задача, характер]\n",
       "8                          [сирия, газопровод, катар, газа, европа, россия, турция, поток, иран, проект]\n",
       "9    [участник, территория, подготовка, прошедшее, житель, исламист, эмиссар, данные, округа, сообщение]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_nx.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Попробуем другие метрики вершин графа, например, closeness_centrality, характеризующую центральность вершины в терминах расстояний до остальных вершин (геометрический центр)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def close_centrality_measure(text, window_size=5, topn=5):\n",
    "    \n",
    "    matrix, id2word = build_matrix(text, window_size)\n",
    "    G = nx.from_numpy_matrix(matrix)\n",
    "    # тут можно поставить любую метрику\n",
    "    node2measure = dict(nx.closeness_centrality(G))\n",
    "    \n",
    "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_between = close_centrality_measure(data.content_norm[0], 7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keywords_between = data['content_norm'].apply(lambda x: close_centrality_measure(x, 7, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.24\n",
      "F1 -  0.15\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, id2word = build_matrix(data.content_norm[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.from_numpy_matrix(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.remove_edges_from(G.selfloop_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 9),\n",
       " (1, 9),\n",
       " (2, 9),\n",
       " (4, 9),\n",
       " (5, 9),\n",
       " (6, 9),\n",
       " (7, 9),\n",
       " (8, 9),\n",
       " (9, 9),\n",
       " (10, 9),\n",
       " (11, 9),\n",
       " (12, 9),\n",
       " (13, 9),\n",
       " (14, 9),\n",
       " (16, 9),\n",
       " (17, 9),\n",
       " (18, 9),\n",
       " (19, 9),\n",
       " (20, 9),\n",
       " (21, 9),\n",
       " (22, 9),\n",
       " (24, 9),\n",
       " (26, 9),\n",
       " (27, 9),\n",
       " (28, 9),\n",
       " (29, 9),\n",
       " (30, 9),\n",
       " (31, 9),\n",
       " (32, 9),\n",
       " (33, 9),\n",
       " (34, 9),\n",
       " (35, 9),\n",
       " (38, 9),\n",
       " (39, 9),\n",
       " (40, 9),\n",
       " (41, 9),\n",
       " (42, 9),\n",
       " (43, 9),\n",
       " (44, 9),\n",
       " (45, 9),\n",
       " (46, 9),\n",
       " (47, 9),\n",
       " (48, 9),\n",
       " (49, 9),\n",
       " (50, 9),\n",
       " (51, 9),\n",
       " (52, 9),\n",
       " (53, 9),\n",
       " (54, 9),\n",
       " (55, 9),\n",
       " (56, 9),\n",
       " (57, 9),\n",
       " (58, 9),\n",
       " (59, 9),\n",
       " (60, 9),\n",
       " (61, 9),\n",
       " (63, 9),\n",
       " (64, 9),\n",
       " (65, 9),\n",
       " (66, 9),\n",
       " (67, 9),\n",
       " (68, 9),\n",
       " (69, 9),\n",
       " (70, 9),\n",
       " (71, 9),\n",
       " (72, 9),\n",
       " (73, 9),\n",
       " (74, 9),\n",
       " (75, 9),\n",
       " (76, 9),\n",
       " (77, 9),\n",
       " (78, 9),\n",
       " (79, 9),\n",
       " (81, 9),\n",
       " (82, 9),\n",
       " (83, 9),\n",
       " (84, 9),\n",
       " (86, 9),\n",
       " (87, 9),\n",
       " (88, 9),\n",
       " (89, 9),\n",
       " (90, 9),\n",
       " (91, 9),\n",
       " (92, 9),\n",
       " (96, 9),\n",
       " (97, 9),\n",
       " (98, 9),\n",
       " (99, 9),\n",
       " (100, 9),\n",
       " (101, 9),\n",
       " (102, 9),\n",
       " (103, 9),\n",
       " (104, 9),\n",
       " (105, 9),\n",
       " (106, 9),\n",
       " (108, 9),\n",
       " (109, 9),\n",
       " (110, 9),\n",
       " (111, 9),\n",
       " (112, 9),\n",
       " (113, 9),\n",
       " (115, 9),\n",
       " (116, 9),\n",
       " (118, 9),\n",
       " (119, 9),\n",
       " (120, 9),\n",
       " (121, 9),\n",
       " (122, 9),\n",
       " (123, 9),\n",
       " (124, 9),\n",
       " (125, 9),\n",
       " (126, 9),\n",
       " (127, 9),\n",
       " (128, 9),\n",
       " (129, 9),\n",
       " (130, 9),\n",
       " (131, 9),\n",
       " (132, 9),\n",
       " (133, 9),\n",
       " (135, 9),\n",
       " (136, 9),\n",
       " (137, 9),\n",
       " (138, 9),\n",
       " (139, 9),\n",
       " (140, 9),\n",
       " (141, 9),\n",
       " (142, 9),\n",
       " (143, 9),\n",
       " (145, 9),\n",
       " (146, 9),\n",
       " (147, 9),\n",
       " (148, 9),\n",
       " (150, 9),\n",
       " (151, 9),\n",
       " (152, 9),\n",
       " (153, 9),\n",
       " (154, 9),\n",
       " (155, 9),\n",
       " (156, 9),\n",
       " (157, 9),\n",
       " (158, 9),\n",
       " (159, 9),\n",
       " (160, 9),\n",
       " (161, 9),\n",
       " (162, 9),\n",
       " (163, 9),\n",
       " (164, 9),\n",
       " (165, 9),\n",
       " (166, 9),\n",
       " (167, 9),\n",
       " (168, 9),\n",
       " (169, 9),\n",
       " (170, 9),\n",
       " (172, 9),\n",
       " (173, 9),\n",
       " (174, 9),\n",
       " (175, 9),\n",
       " (176, 9),\n",
       " (177, 9),\n",
       " (178, 9),\n",
       " (179, 9),\n",
       " (180, 9),\n",
       " (181, 9),\n",
       " (184, 9),\n",
       " (185, 9),\n",
       " (186, 9),\n",
       " (188, 9),\n",
       " (189, 9),\n",
       " (190, 9),\n",
       " (191, 9),\n",
       " (3, 8),\n",
       " (15, 8),\n",
       " (25, 8),\n",
       " (37, 8),\n",
       " (62, 8),\n",
       " (80, 8),\n",
       " (85, 8),\n",
       " (93, 8),\n",
       " (94, 8),\n",
       " (95, 8),\n",
       " (107, 8),\n",
       " (114, 8),\n",
       " (117, 8),\n",
       " (144, 8),\n",
       " (149, 8),\n",
       " (171, 8),\n",
       " (182, 8),\n",
       " (183, 8),\n",
       " (187, 8),\n",
       " (23, 2),\n",
       " (36, 2),\n",
       " (134, 2)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(nx.core_number(G).items(), key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем выделять самые важные вершины с точки зрения значения k-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_k_core(text, window_size=5, topn=5):\n",
    "    \n",
    "    matrix, id2word = build_matrix(text, window_size)\n",
    "    G = nx.from_numpy_matrix(matrix)\n",
    "    G.remove_edges_from(G.selfloop_edges())\n",
    "    \n",
    "    node2measure = nx.core_number(G)\n",
    "    \n",
    "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "kewords_core = get_k_core(data.content_norm[0], 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['люба',\n",
       " 'обязанность',\n",
       " 'месяц',\n",
       " 'факт',\n",
       " 'дело',\n",
       " 'вопрос',\n",
       " 'военный',\n",
       " 'руководство',\n",
       " 'смена',\n",
       " 'сми']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kewords_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kewords_core = data['content_norm'].apply(lambda x: get_k_core(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.03\n",
      "Recall -  0.06\n",
      "F1 -  0.04\n",
      "Jaccard -  0.02\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], kewords_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем представить тексты в виде ориентированного графа и рассмтотреть входящую меру центральности слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oriented_matrix(text, window_size=5):\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            #print(j, k)\n",
    "            m[j][k] += 1\n",
    "    \n",
    "    return m, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_degree_measure(text, window_size=5, topn=5):\n",
    "    \n",
    "    matrix, id2word = oriented_matrix(text, window_size)\n",
    "    G = nx.DiGraph(matrix)\n",
    "    node2measure = nx.in_degree_centrality(G)\n",
    "    \n",
    "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['яблоко',\n",
       " 'акция',\n",
       " 'активист',\n",
       " 'дарья',\n",
       " 'деятельность',\n",
       " 'политика',\n",
       " 'власть',\n",
       " 'выборы',\n",
       " 'убийство',\n",
       " 'март']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_degree_measure(data.content_norm[0], 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kewords_in = data['content_norm'].apply(lambda x: in_degree_measure(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.23\n",
      "F1 -  0.15\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], kewords_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем удалить имена при нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "#     words = [word.strip(punct) for word in text.lower().split()]\n",
    "#     words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    \n",
    "    words = re.findall(r'[a-zа-яё]+', text.lower())\n",
    "   \n",
    "    parsed = [morph.parse(word)[0] for word in words if word and word not in stopwords and len(word)>2]\n",
    "    words = [parsed_word.normal_form for parsed_word in parsed if parsed_word.tag.POS == 'NOUN' and \\\n",
    "             'Name' not in parsed_word.tag]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['content_no_name'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keyword_noname = data['content_no_name'].apply(lambda x: some_centrality_measure(x, 10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.16\n",
      "Recall -  0.21\n",
      "F1 -  0.17\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keyword_noname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [яблоко, акция, политика, активист, деятельность, власть, жизнь]\n",
       "1              [газа, газпром, миллиард, куб, добыча, производитель, холдинг]\n",
       "2                         [книга, жанр, роман, мир, герой, читатель, перевод]\n",
       "3             [видео, журналист, дело, экспертиза, суд, свидетель, заседание]\n",
       "4             [площадь, москва, территория, столица, регион, тинао, развитие]\n",
       "5                    [гонка, позиция, команда, место, чемпионат, круг, пилот]\n",
       "6                [место, влияние, позиция, глава, россия, президент, участие]\n",
       "7      [культура, закон, сфера, сообщество, учреждение, концепция, изменение]\n",
       "8                    [газопровод, сирия, россия, турция, катар, газа, европа]\n",
       "9    [участник, территория, подготовка, прошедшее, житель, исламист, эмиссар]\n",
       "Name: content_no_name, dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_noname.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшое изменение предобработки немного повысило f-меру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.1177654755913435"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['keywords'].apply(lambda x: len(x)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем так же метрику betweenness_centrality, характеризующую степень того, насколько часто через вершину проходят кратчайшие пути между другими вершинами графа (слова, которые часто встречаются в разных контекстах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def between_centrality_measure(text, window_size=5, topn=5):\n",
    "    \n",
    "    matrix, id2word = build_matrix(text, window_size)\n",
    "    G = nx.from_numpy_matrix(matrix)\n",
    "    # тут можно поставить любую метрику\n",
    "    node2measure = dict(nx.betweenness_centrality(G))\n",
    "    \n",
    "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c66640ebbaf4e56a04f3f3fe65d3ec8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 22min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keyword_betw = [between_centrality_measure(x, 10, 7) for x in tqdm_notebook(data['content_no_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.15\n",
      "Recall -  0.21\n",
      "F1 -  0.16\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keyword_betw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В семинаре установлен такой бейзлан - F1 -  0.16 (не будем учитывать точность и полноту по отдельности и отбросим жаккара)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ваша задача - предложить 3 способа побить бейзлайн. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет никаких ограничений кроме:\n",
    "\n",
    "1) нельзя изменять метрику\n",
    "2) решение должно быть воспроизводимым"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве ответа нужно предоставить jupyter тетрадку с экспериментами (обязательное условие!) и описать каждую из идей в форме - https://goo.gl/forms/Zb89yjXFr37EITMq1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый реализованный и описанный способ оценивается в 3 балла. Дополнительный балл можно получить, если способы затрагивают разные аспекты решения (например, первая идея - улучшить нормализацию, вторая - улучшить способ представления текста в виде графа, третья - предложить способ удаления из топа идентичных ключевых слов (рф, россия))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать мой код как основу, а можно придумать что-то полностью другой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас никак не получается побить бейзлайн вы можете предоставить реализацию и описание неудавшихся экспериментов (каждый оценивается в 0.5 баллов)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
