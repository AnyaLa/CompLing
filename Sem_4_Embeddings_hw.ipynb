{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\Program Files\\Anaconda3\\Lib\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter,defaultdict\n",
    "from string import punctuation\n",
    "import os\n",
    "# from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "# stops = set(stopwords.words('russian'))\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stops = get_stop_words('russian')\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус новостей для обучений векторных моеделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt = pd.read_csv('news_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7217, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7212, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Канцлер Германии Ангела Меркель в ходе брифинг...</td>\n",
       "      <td>канцлер германия ангел меркель ход брифинг пре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Российские и белорусские войска успешно заверш...</td>\n",
       "      <td>российский белорусский войско успешно завершит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Дзюба, Шатов и Анюков оказались не нужны «Зени...</td>\n",
       "      <td>дзюба шат анюк оказаться нужный зенит российск...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Испанию без фанатов\\nПожалуй, главной пятнич...</td>\n",
       "      <td>испания фанат пожалуй главный пятничный новост...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Постпред России при ООН Виталий Чуркин, говоря...</td>\n",
       "      <td>постпред россия оон виталий чуркин говорить ве...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Канцлер Германии Ангела Меркель в ходе брифинг...   \n",
       "1  Российские и белорусские войска успешно заверш...   \n",
       "2  Дзюба, Шатов и Анюков оказались не нужны «Зени...   \n",
       "3  В Испанию без фанатов\\nПожалуй, главной пятнич...   \n",
       "4  Постпред России при ООН Виталий Чуркин, говоря...   \n",
       "\n",
       "                                        content_norm  \n",
       "0  канцлер германия ангел меркель ход брифинг пре...  \n",
       "1  российский белорусский войско успешно завершит...  \n",
       "2  дзюба шат анюк оказаться нужный зенит российск...  \n",
       "3  испания фанат пожалуй главный пятничный новост...  \n",
       "4  постпред россия оон виталий чуркин говорить ве...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=10000)\n",
    "X = tfidf.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7212, 10000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.88 s, sys: 5.49 s, total: 11.4 s\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svd = TruncatedSVD(50)\n",
    "svd.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.1 s, sys: 40.7 s, total: 1min 30s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nmf = NMF(50)\n",
    "nmf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:w for i,w in enumerate(cv.get_feature_names())}\n",
    "word2id = {w:i for i,w in id2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vec_svd = nmf.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vec_nmf = svd.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, id2vec):\n",
    "    similar = [id2word[i] for i in cosine_distances(id2vec[word2id[word]].reshape(1, -1), id2vec).argsort()[0][:10]]\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['книга',\n",
       " 'писатель',\n",
       " 'творчество',\n",
       " 'искусство',\n",
       " 'произведение',\n",
       " 'описывать',\n",
       " 'знаменитый',\n",
       " 'повесть',\n",
       " 'поэт',\n",
       " 'xx']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('книга', id2vec_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['книга',\n",
       " 'искусство',\n",
       " 'писатель',\n",
       " 'произведение',\n",
       " 'знаменитый',\n",
       " 'художник',\n",
       " 'лауреат',\n",
       " 'премия',\n",
       " 'творчество',\n",
       " 'описать']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('книга', id2vec_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BaseWordEmbeddingsModel', 'FASTTEXT_FILEFORMAT_MAGIC', 'FAST_VERSION', 'FastText', 'FastTextKeyedVectors', 'FastTextTrainables', 'FastTextVocab', 'MAX_WORDS_IN_BATCH', 'REAL', 'Vocab', 'Word2VecTrainables', 'Word2VecVocab', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_compute_ngrams', '_ft_hash', 'call_on_class_only', 'deprecated', 'empty', 'logger', 'logging', 'np', 'np_sum', 'ones', 'struct', 'train_batch_cbow', 'train_batch_sg', 'train_cbow_pair', 'train_sg_pair', 'utils', 'vstack']\n"
     ]
    }
   ],
   "source": [
    "print(dir(fasttext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 51s, sys: 880 ms, total: 3min 51s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = gensim.models.Word2Vec([text.split() for text in data_rt['content_norm']], size=50, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 34s, sys: 1.54 s, total: 8min 36s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fast_text = fasttext.FastText([text.split() for text in data_rt['content_norm']], size=50, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alapidus/miniconda3/envs/compling/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('мультфильм', 0.8800262212753296),\n",
       " ('кино', 0.8518549203872681),\n",
       " ('фильтр', 0.8429263234138489),\n",
       " ('фильм', 0.8424779176712036),\n",
       " ('мультсериал', 0.819197416305542),\n",
       " ('3234', 0.8115113377571106),\n",
       " ('кинолента', 0.8103165626525879),\n",
       " ('фил', 0.8021842837333679),\n",
       " ('кинематограф', 0.7988088130950928),\n",
       " ('сериал', 0.7928453683853149)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text.most_similar('книга')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alapidus/miniconda3/envs/compling/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('произведение', 0.8490647077560425),\n",
       " ('поэма', 0.834342896938324),\n",
       " ('литературный', 0.8266890048980713),\n",
       " ('картина', 0.8174421787261963),\n",
       " ('стихотворение', 0.7986379861831665),\n",
       " ('мультфильм', 0.7961488366127014),\n",
       " ('издательство', 0.7959349155426025),\n",
       " ('комикс', 0.7936421632766724),\n",
       " ('автобиография', 0.7908923625946045),\n",
       " ('титаник', 0.7905163764953613)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('книга')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Корпус парафразов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.4 s, sys: 0 ns, total: 27.4 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
       "      <td>полиция разрешить стрелять хулиган травматика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>право полицейский проникновение жилища решить ...</td>\n",
       "      <td>правило внесудебный проникновение полицейский ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>президент египет ввести чрезвычайный положение...</td>\n",
       "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Приставы соберут отпечатки пальцев российских ...</td>\n",
       "      <td>Приставы снимут отпечатки пальцев у злостных н...</td>\n",
       "      <td>пристав собрать отпечаток палец российский дол...</td>\n",
       "      <td>пристав снять отпечаток палец злостный неплате...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>На саратовского дебошира с борта самолета Моск...</td>\n",
       "      <td>Саратовский дебошир отказывается возвращаться ...</td>\n",
       "      <td>саратовский дебошир борт самолёт москва хургад...</td>\n",
       "      <td>саратовский дебошир отказываться возвращаться ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>ЦИК хочет отказаться от электронной системы по...</td>\n",
       "      <td>ЦИК может отказаться от электронных средств по...</td>\n",
       "      <td>цик хотеть отказаться электронный система подс...</td>\n",
       "      <td>цик отказаться электронный средство подсчёт голос</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>Суд Петербурга оставил на потом дело о гибели ...</td>\n",
       "      <td>Лондонский Гайд-парк - это не место для митинг...</td>\n",
       "      <td>суд петербург оставить дело гибель подросток п...</td>\n",
       "      <td>лондонский гайд-парк место митинг прежде парк</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>Страны ОПЕК сократили добычу нефти на 1 млн ба...</td>\n",
       "      <td>Обама продлил полномочия НАСА по сотрудничеств...</td>\n",
       "      <td>страна опека сократить добыча нефть 1 миллион ...</td>\n",
       "      <td>обама продлить полномочие наса сотрудничество ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             text_1  \\\n",
       "0     0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1     0  Право полицейских на проникновение в жилище ре...   \n",
       "2     0  Президент Египта ввел чрезвычайное положение в...   \n",
       "3    -1  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4     0  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "5     1  Приставы соберут отпечатки пальцев российских ...   \n",
       "6    -1  На саратовского дебошира с борта самолета Моск...   \n",
       "7     0  ЦИК хочет отказаться от электронной системы по...   \n",
       "8    -1  Суд Петербурга оставил на потом дело о гибели ...   \n",
       "9    -1  Страны ОПЕК сократили добычу нефти на 1 млн ба...   \n",
       "\n",
       "                                              text_2  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...   \n",
       "1  Правила внесудебного проникновения полицейских...   \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...   \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...   \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...   \n",
       "5  Приставы снимут отпечатки пальцев у злостных н...   \n",
       "6  Саратовский дебошир отказывается возвращаться ...   \n",
       "7  ЦИК может отказаться от электронных средств по...   \n",
       "8  Лондонский Гайд-парк - это не место для митинг...   \n",
       "9  Обама продлил полномочия НАСА по сотрудничеств...   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский разрешить стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилища решить ...   \n",
       "2  президент египет ввести чрезвычайный положение...   \n",
       "3  вернуться сирия россиянин волновать вопрос тру...   \n",
       "4  москва сирия вернуться 2 самолёт мчс россиянин...   \n",
       "5  пристав собрать отпечаток палец российский дол...   \n",
       "6  саратовский дебошир борт самолёт москва хургад...   \n",
       "7  цик хотеть отказаться электронный система подс...   \n",
       "8  суд петербург оставить дело гибель подросток п...   \n",
       "9  страна опека сократить добыча нефть 1 миллион ...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0      полиция разрешить стрелять хулиган травматика  \n",
       "1  правило внесудебный проникновение полицейский ...  \n",
       "2  власть египет угрожать ввести страна чрезвычай...  \n",
       "3      самолёт мчс вывезти россиянин разрушить сирия  \n",
       "4      самолёт мчс вывезти россиянин разрушить сирия  \n",
       "5  пристав снять отпечаток палец злостный неплате...  \n",
       "6  саратовский дебошир отказываться возвращаться ...  \n",
       "7  цик отказаться электронный средство подсчёт голос  \n",
       "8      лондонский гайд-парк место митинг прежде парк  \n",
       "9  обама продлить полномочие наса сотрудничество ...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_text_1 = svd.transform(tfidf.transform(data['text_1_norm']))\n",
    "svd_text_2 = svd.transform(tfidf.transform(data['text_2_norm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 50)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_text_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79504575]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(svd_text_1[0].reshape(1, -1), svd_text_2[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_sim = cosine_similarity(svd_text_1, svd_text_2).diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79504575, 0.81161594, 0.76032713, 0.63506641, 0.98274664,\n",
       "       0.39290103, 0.47727611, 0.8832077 , 0.29858323, 0.13979391])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_sim[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_text_1 = nmf.transform(tfidf.transform(data['text_1_norm']))\n",
    "nmf_text_2 = nmf.transform(tfidf.transform(data['text_2_norm']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_sim = cosine_similarity(nmf_text_1, nmf_text_2).diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96485237, 0.92440748, 0.86394129, 0.53192501, 0.99169581,\n",
       "       0.51535688, 0.33571858, 0.8416168 , 0.13748259, 0.0679591 ])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_sim[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim):\n",
    "    text = text.split()\n",
    "    \n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v*(words[word]/total) # просто умножаем вектор на частоту\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alapidus/miniconda3/envs/compling/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "w2v_text_1 = np.zeros((len(data['text_1_norm']), dim))\n",
    "w2v_text_2 = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    w2v_text_1[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    w2v_text_2[i] = get_embedding(text, w2v, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 50)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_text_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sim = cosine_similarity(w2v_text_1, w2v_text_2).diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90944059, 0.91393835, 0.95933313, 0.72613995, 0.90710678,\n",
       "       0.93119273, 0.78634835, 0.97720843, 0.64907291, 0.6467132 ])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_sim[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alapidus/miniconda3/envs/compling/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "fast_text_1 = np.zeros((len(data['text_1_norm']), dim))\n",
    "fast_text_2 = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    fast_text_1[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    fast_text_2[i] = get_embedding(text, fast_text, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_sim = cosine_similarity(fast_text_1, fast_text_2).diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80669522, 0.82055602, 0.83609194, 0.62733576, 0.69889539,\n",
       "       0.77904897, 0.38122067, 0.88573169, 0.19446539, 0.14171555])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_sim[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['svd_sim'] = svd_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nmf_sim'] = nmf_sim\n",
    "data['w2v_sim'] = w2v_sim\n",
    "data['fasttext_sim'] = fasttext_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load(\"out.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_adagram(text, model, window, dim):\n",
    "    text = text.split()\n",
    "    \n",
    "    \n",
    "    word2context = []\n",
    "    for i in range(len(text)-1):\n",
    "        left = max(0, i-window)\n",
    "        word = text[i]\n",
    "        left_context = text[left:i]\n",
    "        right_context = text[i+1:i+window]\n",
    "        context = left_context + right_context\n",
    "        word2context.append((word, context))\n",
    "    \n",
    "    \n",
    "    \n",
    "    vectors = np.zeros((len(word2context), dim))\n",
    "    \n",
    "    for i,word in enumerate(word2context):\n",
    "        word, context = word\n",
    "        try:\n",
    "            sense = model.disambiguate(word, context).argmax()\n",
    "            v = model.sense_vector(word, sense)\n",
    "            vectors[i] = v # просто умножаем вектор на частоту\n",
    "        \n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "adagr_text_1 = np.zeros((len(data['text_1_norm']), dim))\n",
    "adagr_text_2 = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    adagr_text_1[i] = get_embedding_adagram(text, vm, 5, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    adagr_text_2[i] = get_embedding_adagram(text, vm, 5, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagram_sim = cosine_similarity(adagr_text_1, adagr_text_2).diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['adagram_sim'] = adagram_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "      <th>svd_sim</th>\n",
       "      <th>nmf_sim</th>\n",
       "      <th>w2v_sim</th>\n",
       "      <th>fasttext_sim</th>\n",
       "      <th>adagram_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
       "      <td>полиция разрешить стрелять хулиган травматика</td>\n",
       "      <td>0.795046</td>\n",
       "      <td>0.964852</td>\n",
       "      <td>0.909441</td>\n",
       "      <td>0.806695</td>\n",
       "      <td>0.734838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>право полицейский проникновение жилища решить ...</td>\n",
       "      <td>правило внесудебный проникновение полицейский ...</td>\n",
       "      <td>0.811616</td>\n",
       "      <td>0.924407</td>\n",
       "      <td>0.913938</td>\n",
       "      <td>0.820556</td>\n",
       "      <td>0.787086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>президент египет ввести чрезвычайный положение...</td>\n",
       "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
       "      <td>0.760327</td>\n",
       "      <td>0.863941</td>\n",
       "      <td>0.959333</td>\n",
       "      <td>0.836092</td>\n",
       "      <td>0.754871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "      <td>0.635066</td>\n",
       "      <td>0.531925</td>\n",
       "      <td>0.726140</td>\n",
       "      <td>0.627336</td>\n",
       "      <td>0.361842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "      <td>0.982747</td>\n",
       "      <td>0.991696</td>\n",
       "      <td>0.907107</td>\n",
       "      <td>0.698895</td>\n",
       "      <td>0.560512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             text_1  \\\n",
       "0     0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1     0  Право полицейских на проникновение в жилище ре...   \n",
       "2     0  Президент Египта ввел чрезвычайное положение в...   \n",
       "3    -1  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4     0  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...   \n",
       "1  Правила внесудебного проникновения полицейских...   \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...   \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...   \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский разрешить стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилища решить ...   \n",
       "2  президент египет ввести чрезвычайный положение...   \n",
       "3  вернуться сирия россиянин волновать вопрос тру...   \n",
       "4  москва сирия вернуться 2 самолёт мчс россиянин...   \n",
       "\n",
       "                                         text_2_norm   svd_sim   nmf_sim  \\\n",
       "0      полиция разрешить стрелять хулиган травматика  0.795046  0.964852   \n",
       "1  правило внесудебный проникновение полицейский ...  0.811616  0.924407   \n",
       "2  власть египет угрожать ввести страна чрезвычай...  0.760327  0.863941   \n",
       "3      самолёт мчс вывезти россиянин разрушить сирия  0.635066  0.531925   \n",
       "4      самолёт мчс вывезти россиянин разрушить сирия  0.982747  0.991696   \n",
       "\n",
       "    w2v_sim  fasttext_sim  adagram_sim  \n",
       "0  0.909441      0.806695     0.734838  \n",
       "1  0.913938      0.820556     0.787086  \n",
       "2  0.959333      0.836092     0.754871  \n",
       "3  0.726140      0.627336     0.361842  \n",
       "4  0.907107      0.698895     0.560512  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('paraphrase_sim.csv', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['svd_sim', 'nmf_sim', 'w2v_sim', 'fasttext_sim','adagram_sim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.67      0.65      0.66       642\n",
      "          0       0.53      0.47      0.50       758\n",
      "          1       0.43      0.55      0.48       407\n",
      "\n",
      "avg / total       0.56      0.55      0.55      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, random_state=14)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_leaf=10,\n",
    "                             class_weight='balanced')\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5404689227411735\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "f1_scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # print(train_index, test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_leaf=10,\n",
    "                             class_weight='balanced')\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    f1_scores.append(f1_score(y_test, preds, average='micro'))\n",
    "    \n",
    "print(np.mean(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5491887416426176\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "f1_scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # print(train_index, test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = LogisticRegression(C=1000, class_weight='balanced',  multi_class='ovr')\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    f1_scores.append(f1_score(y_test, preds, average='micro'))\n",
    "    \n",
    "print(np.mean(f1_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И для логистической регрессии, и для RandomForest средняя F1-мера при микроусреднении - около 54%. Для логистической регрессии результат чуть лучше.\n",
    "Попробуем добавить косинусную близость между TFIDF-векторами в качестве признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_1 = tfidf.transform(data['text_1_norm'])\n",
    "tfidf_text_2 = tfidf.transform(data['text_2_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_sim = cosine_similarity(tfidf_text_1, tfidf_text_2).diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49287979, 0.67081215, 0.84998217, 0.15270967, 0.60388876,\n",
       "       0.66088505, 0.39221454, 0.92201252, 0.        , 0.        ,\n",
       "       0.77863067, 0.        , 0.37664076, 0.65858434, 0.79328721,\n",
       "       0.63402837, 0.50096436, 0.82150569, 0.46494595, 0.65304967])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sim[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tfidf_sim'] = tfidf_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['svd_sim', 'nmf_sim', 'w2v_sim', 'fasttext_sim','adagram_sim', 'tfidf_sim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5520967518078749\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "f1_scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # print(train_index, test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = LogisticRegression(C=1000, class_weight='balanced',  multi_class='ovr')\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    f1_scores.append(f1_score(y_test, preds, average='micro'))\n",
    "    \n",
    "print(np.mean(f1_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-мера поднялась до 55% :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем добавить tfidf взвешивание для w2v и fasttext представлений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.39057879, 5.35104074, 5.07961919, ..., 5.60697411, 5.91334832,\n",
       "       6.35727971])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dict = dict([(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_embedding(text, model, dim, idf):\n",
    "    text = text.split()\n",
    "    \n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    \n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v*(words[word]/total)*idf[word] # просто умножаем вектор на частоту\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alapidus/miniconda3/envs/compling/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "fasttfidf_text_1 = np.zeros((len(data['text_1_norm']), dim))\n",
    "fasttfidf_text_2 = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    fasttfidf_text_1[i] = tfidf_embedding(text, fast_text, dim,idf_dict)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    fasttfidf_text_2[i] = tfidf_embedding(text, fast_text, dim,idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alapidus/miniconda3/envs/compling/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "w2vtfidf_text_1 = np.zeros((len(data['text_1_norm']), dim))\n",
    "w2vtfidf_text_2 = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    w2vtfidf_text_1[i] = tfidf_embedding(text, w2v, dim,idf_dict)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    w2vtfidf_text_2[i] = tfidf_embedding(text, w2v, dim,idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttfidf_sim = cosine_similarity(fasttfidf_text_1, fasttfidf_text_2).diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84091925, 0.89149951, 0.85495967, 0.66555334, 0.92271267,\n",
       "       0.71993618, 0.60916878, 0.95228467, 0.33120287, 0.17634337])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttfidf_sim[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80669522, 0.82055602, 0.83609194, 0.62733576, 0.69889539,\n",
       "       0.77904897, 0.38122067, 0.88573169, 0.19446539, 0.14171555])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_sim[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vtfidf_sim = cosine_similarity(w2vtfidf_text_1, w2vtfidf_text_2).diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fasttfidf_sim'] = fasttfidf_sim\n",
    "data['w2vtfidf_sim'] = w2vtfidf_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['svd_sim', 'nmf_sim', 'w2v_sim', 'fasttext_sim','adagram_sim', 'tfidf_sim', 'fasttfidf_sim', 'w2vtfidf_sim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5478070515489575\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "f1_scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # print(train_index, test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = LogisticRegression(C=1000, class_weight='balanced',  multi_class='ovr')\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    f1_scores.append(f1_score(y_test, preds, average='micro'))\n",
    "    \n",
    "print(np.mean(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1-мера осталась на том же уровне."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
